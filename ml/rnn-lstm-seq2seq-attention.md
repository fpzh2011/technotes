# RNN LSTM seq2seq Attention

## 资料汇编

[seq2seq学习笔记](https://blog.csdn.net/Jerr__y/article/details/53749693)
[递归神经网络不可思议的有效性](https://www.csdn.net/article/2015-08-28/2825569)
[理解 LSTM 网络](https://blog.csdn.net/jerr__y/article/details/58598296)
[深度学习中的注意力机制](https://mp.weixin.qq.com/s?__biz=MzA4Mzc0NjkwNA==&mid=2650783542&idx=1&sn=3846652d54d48e315e31b59507e34e9e&chksm=87fad601b08d5f17f41b27bb21829ed2c2e511cf2049ba6f5c7244c6e4e1bd7144715faa8f67&mpshare=1&scene=1&srcid=1113JZIMxK3XhM9ViyBbYR76#rd)
[深度学习中的注意力机制](https://blog.csdn.net/malefactor/article/details/78767781)
[自然语言处理中的Attention Model：是什么及为什么](https://blog.csdn.net/malefactor/article/details/50550211)
[注意力与记忆机制](https://nndl.github.io/chap-%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8E%E8%AE%B0%E5%BF%86%E6%9C%BA%E5%88%B6.pdf)
[使用RNN解决NLP中序列标注问题的通用优化思路](https://blog.csdn.net/malefactor/article/details/50725480)
[深度学习与自然语言处理之五：从RNN到LSTM](https://blog.csdn.net/malefactor/article/details/50436735)
[张俊林的博客](https://blog.csdn.net/malefactor)

## github代码

递归神经网络不可思议的有效性
https://github.com/karpathy/char-rnn
seq2seq学习笔记
https://github.com/nicolas-ivanov/tf_seq2seq_chatbot

